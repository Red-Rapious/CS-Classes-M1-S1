\newpage

\section{Generative and Autoregressive Models}
\subsection{Generative Models}
\subsubsection{Discriminative vs Generative models}
A \emph{discriminative model} is the most common type of supervised learning model: for any input $x$, it predicts its label $y$. Probabilistically speaking, a dscriminative model learns the conditional probability distribution $\P(y|x)$, that is the probability for each label to correspond to the input. 

A \emph{generative model} learns the probability distribution $\P(x)$, or in the case of a \emph{conditional generative model}, the conditional distribution $\P(x|y)$, which present a huge mathematical and practical distinction with discriminative models.

Note that a discriminative model has no way to handle \say{unreasonable inputs}: if an input does not fit the training distribution, the model still needs to output a probability distribution over the outputs set. Nevertheless, generative models can predict the likeliness of an input to belong to the input distribution: it can \say{reject} unreasonable inputs by assigning small probabilities to them.

According to Bayes' rule, it is possible to build a conditional generative model from other components:
\begin{equation*}
    \underbrace{\P(x|y)}_{\substack{\text{Conditional}\\ \text{Generative Model}}} = \frac{\overbrace{\P(y|x)}^{\text{Discriminative Model}}}{\underbrace{\P(y)}_{\text{Prior over labels}}}\cdot\underbrace{\P(x)}_{\substack{\text{Unconditional}\\ \text{Generative Model}}}
\end{equation*}
This shows that building an unconditional generative model provides a conditional one without additional complexity.

Generative models can be used for a variety of tasks, including the detection of unlikely inputs, features learning and the generation of new input data by sampling from the learned distribution.

\subsubsection{Taxonomy of generative models}
Various types of generative models have emerged. The main distinctions can be made between models that can compute $\P(x)$ (explicit density models) and models that can only sample from it (implicit density models).
\begin{figure}[H]
    \centering
    \includegraphics[width=.75\textwidth]{autoencoders/taxonomy-generative.png} 
    \caption{Taxonomy of generative models}
\end{figure}
In this chapter, we will study an explicit and tractable density model family, the autoregressive models. In the next chapters, we will introduce Variational Autoencoders and Generative Adversarial Networks. 

\subsection{Autoregressive Models}
\subsubsection{Explicit density estimation}
Our goal is to write an explicit function for the likelihood $\P(x)$, that is to find a parametric function $f_\theta$ such that for a certain learned value of $\theta^*$,
\begin{equation*}
    \forall x,\quad \P(x) = f_{\theta^*}(x)
\end{equation*}
Given a label-less dataset $(x^{(1)}, \dots, x^{(N)})$, we can train the model by solving the equation:
\begin{equation*}
    \begin{aligned}
        \theta^* &:= \argmax_\theta \prod_{i=1}^N \P(x^{(i)})\\
        &= \argmax_\theta \sum_{i=1}^N \log\P(x^{(i)})\\
        &= \argmax_\theta \sum_{i=1}^N \log f_\theta(x^{(i)})
    \end{aligned}
\end{equation*}
corresponding to maximum likelihood estimation. Therefore, we can simply take:
\begin{equation*}
    \L : \theta \longmapsto \sum_{i=1}^N \log f_\theta(x^{(i)})
\end{equation*}
as a loss function, and minimize it using gradient descent. In most practical applications, $\left(f_\theta\right)_\theta$ is a family of neural networks.

\subsubsection{Explicit density estimation using a regressive model}
The specificity of autoregressive models is to assume that each training sample $x^{(i)}$ consists of multiple subparts $(x_1^{(i)}, x_2^{(i)}, \dots, x_T^{(i)})$. For instance, in the case of images, each subpart can be a specific pixel.

We can break down the probability of observing a specific input by using the probability chain rule:
\begin{equation*}
    \begin{aligned}
        \P(x) &= \P(x_1, x_2, \dots, x_T)\\
        &= \P(x_1) \cdot \P(x_2|x_1) \cdot \P(x_3|x_1, x_2)\cdot \dots\\
        &= \prod_{t=1}^T \P(x_t|x_1, \dots, x_{t-1})
    \end{aligned}
\end{equation*}
This type of dependency is extremely similar to the problem solved by recurrent neural network: we want each prediction $\P(x_t|x_1, \dots, x_{t-1})$ to be conditioned on the previous time steps $x_1, \dots, x_{t-1}$. Therefore, we can train a recurrent neural network to output the successive probabilities $\P(x_t|x_1, \dots, x_{t-1})$ in a meaningful order, which can then be multiplied to obtain $\P(x)$.

\subsubsection{PixelRNN}
This is the idea of a PixelRNN: a generative model with an explicit and tractable density function, that predicts pixel values one at a time.

For each pixel, we compute a hidden state of the RNN, that depends on the hidden states and RGB values of pixels left and above:
\begin{equation*}
    h_{x,y} = f_\theta(h_{x-1,y}, h_{x, y-1})
\end{equation*}
That way, each pixel depends implicitly on all pixels left and above.
At each pixel, we can then predict the red, green, and blue values, by computing a probability distribution over $\iset{0}{255}$. 
\begin{figure}[H]
    \centering
    \includegraphics[width=.4\textwidth]{autoencoders/pixelrnn.png} 
    \caption{Dependency of pixels in PixelRNN.}
\end{figure}

The major issue of this generative approach is that is it very slow both during training and testing.

\subsubsection{PixelCNN}
To overcome the performance issues of PixelRNN, another architecture called PixelCNN has been introduced: it still generates pixels from the top-left corner, but the dependency on each pixel is now modeled using a masked convolution over previously computed regions.
\begin{figure}[H]
    \centering
    \includegraphics[width=.5\textwidth]{autoencoders/pixelcnn.png} 
    \caption{Dependency of pixels in PixelCNN.}
\end{figure}
This approach allows for parallel computations of dependencies over previous pixels, making it much faster to train. It remains quite slow, because the generation must still proceed sequentially.

Generated result show that the model learned the high-level structure of the input images, by providing features such as edges and flat color shapes, but the images do not represent anything real when looked closely.\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{autoencoders/pixelcnn-results.png} 
    \caption{Results of PixelCNN.}
\end{figure}

\subsubsection{Beyond autoregressive models}
Autoregressive models have the benefit of explicitly computing the likelihood $\P(x)$, making them very easy to evaluate. Nevertheless, they require a sequential generation, making the sampling process slow.

The performance of PixelCNN can be improved with tricks in practice, such as the use of multiscale generation, gated convolutional layers or shortcut connections. 

\newpage
\section{Autoencoders}
An \emph{autoencoder} is a type of neural network used to solve unsupervised learning problems. The main idea of an autoencoder is to learn an efficient representation of the input data, usually of the form of a hidden layer of size smaller than the input size. It can be useful in dimensionality reduction, but the mostly used variant is the \emph{variational autoencoder}, a certain type of generative model.

\subsection{Regular Autoencoders}
Autoencoders are designed to learn feature vectors from raw data, without any labels. Features should extract useful information, that we can use for other tasks. The main challenge is to learn this feature transform from raw data, without any idea of the form that this feature vector should take. Therefore, the idea of autoencoders is to ask the model to use these features to reconstruct the raw data; this way, the model will keep the features that help it reconstruct the data, which can be intuitively considered as the \say{best} features.

An autoencoder consists of two parts: an encoder, taking into input the raw data and outputing the feature vector; and a decoder, taking the feature vector as input and outputing the reconstructing input data. Both the encoder and the decoder are updated during training; the name \emph{autoencoder} shows that the model chooses its own code.
\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{autoencoders/encoder-decoder.png}
    \caption{The encoder-decoder architecture of an autoencoder.}
\end{figure}
Since the goal of the decoder is to reconstruct the input data as well as possible, our loss function will be the difference between the input and the output data:
\begin{equation*}
    \L(x) = \norm{f_\theta(x)-x}_2^2
\end{equation*}

In general, learning the identity function is not especially usefull and can be easily learned by a simple neural network. Therefore, features need to have a low dimension compared to the data. If the model approximates the identity function well, it means that the latent code (the feature vector) is an efficient way to represent the input data.



\newpage