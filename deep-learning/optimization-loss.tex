\section{Optimization and loss functions}
\subsection{Tensors in \texttt{PyTorch}}
A tensor is a $d$-dimensional array in \texttt{PyTorch}. Tensors are used in deep learning to represent all kind of data, from images to weight matrices. 

\paragraph*{Tensor creation}
A tensor can be created from a list: \mintinline{python}{x = torch.Tensor([[1,0,2],[3,2,2]])}. By default, tensors are not deepcopied, but can be cloned: \mintinline{python}{x.clone()}. Each tensor has a data type, which can be specified at its creation: \mintinline{python}{x = torch.Tensor(..., dtype=torch.int64)}. In the case of data (for instance, training examples), the first dimension of a tensor is usually the samples: \mintinline{python}{x.shape[0]} is the batch size.

\paragraph*{Operations}
Most operations on tensors (\mintinline{python}{+}, \mintinline{python}{*}, \dots) are performed coordinate-wise and need matching sizes. Mathematical functions from the \mintinline{python}{torch} library, such as \mintinline{python}{torch.exp} or \mintinline{python}{x**2}, are vectorized and therefore performed coordinate-wise. Notably, matrix multiplication can be perfomed using the \mintinline{python}{x @ y} syntax.

\paragraph*{Shapes}
A tensor shape can be obtained by calling \mintinline{python}{x.shape}. Reshaping can be performed using \mintinline{python}{x.view(1,3,-1)}, where \mintinline{python}{-1} acts as a wildcard for \texttt{PyTorch} to fill in automatically the appropriate dimension. For instance, \mintinline{python}{x.view(-1)} flattens the tensor into a one-dimensional vector. The operation \mintinline{python}{x.unsqueeze(0)} adds a dimension of size 1 to the tensor.

\paragraph*{Gradients}
Tensors can have an associated gradient, stored in \mintinline{python}{x.grad}. We can remove (and clone) this tensor from the computation of the gradient by using \mintinline{python}{y = x.detach()}.

\paragraph*{Other operations}
See the \href{https://pytorch.org/docs/stable/tensors.html}{\texttt{PyTorch} documentation} for numerous operations defined on tensors. Most convenient ones include \mintinline{python}{torch.sum(x)}, \mintinline{python}{torch.mean(x)}. A tensor can also be converted to a \texttt{NumPy} array using \mintinline{python}{x.numpy()} or \mintinline{python}{x.detach().numpy()}.

Manipulating tensors and tensor sizes is complex and leads to many bugs in deep learning projects. Many errors can go unnoticed due to wrong tensor sizes and Python's dynamic typing. As a general advice, always verify your intermediate computations using for instance \mintinline{python}{print(x[:5])}, and your tensor shapes with \mintinline{python}{print(x.shape)}!

\subsection{Loss functions}
\subsubsection{Mean Square Error}
\begin{definition}[Mean Square Error]
    The \emph{mean square error} is the loss function defined by:
    \begin{equation*}
        \begin{aligned}
            \ell : \R^d\times\R^d&\longrightarrow\R\\
            x, y&\longmapsto \norm{x-y}_2^2 = (x-y)^\tp(x-y)
        \end{aligned}
    \end{equation*}
\end{definition}

MSE has a probabilistic interpretation, fitting the following probabilistic model: we are trying to learn a certain function $g_\theta$, parametered by $\theta\in\R^p$. To do so, we are given tuples of data of the form $(X_i, Y_i)$, where $Y_i$ is the label corresponding to $X_i$. We assume that the probabilistic relationship between $X_i$ and $Y_i$ is the following:
\begin{equation*}
    Y_i=g_\theta(X_i)+\epsilon_i
\end{equation*}
where $\epsilon_i\sim\mathcal{N}(0,\sigma^2I_d)$ are i.i.d.~centered Gaussian random variables (that is of mean 0 and variance $\sigma^2$). We also assume that the $X_i$ are i.i.d.~and independent of $\theta$.

Let's apply the Maximum Likelihood principle to this probabilistic model. The likelihood for the data to be drawn from a given $\theta$ is:
\begin{equation*}
    \begin{aligned}
        \P_\theta((X_i,Y_i)_i)&=\prod_i \P(X_i) \cdot \P_\theta\left(\epsilon_i=Y_i-g_\theta(X_i)\right)\\
        &=\prod_i \P(X_i) \cdot \frac{1}{\sqrt{(2\pi)^d|\sigma^2I_d|}}\cdot\exp\left(-\frac{1}{2}(Y_i-g_\theta(X_i)-0)^\tp(\sigma^2I_d)^{-1}(Y_i-g_\theta(X_i)-0)\right) \\
        &=\prod_i \frac{\P(X_i)}{\sqrt{(2\pi\sigma^2)^d}}\cdot\exp\left(-\frac{1}{2\sigma^2}\norm{Y_i-g_\theta(X_i)}_2^2\right) \\
        &\propto\exp\left(-\frac{\sum_i\norm{Y_i-g_\theta(X_i)}_2^2}{2\sigma^2}\right)\\
        &= \exp\left(-\frac{\ell(Y_i,g_\theta(X_i))}{2\sigma^2}\right)
    \end{aligned}
\end{equation*}
Therefore, maximizing the $\log$-likelihood is equivalent to minimizing the MSE.

In \texttt{PyTorch}, means square error can be used with the line \mintinline{python}{criterion = nn.MSELoss()}. It supports several parameters, such as \mintinline{python}{reduction='sum'}, \mintinline{python}{reduction='mean'}, which are described in \href{https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html}{the documentation}. Once set, \mintinline{python}{criterion} takes as input two tensors of shapes $[N,d]$, where $N$ is the batch size and $d$ is the dimension of the output vectors. Note that when $d=1$, the output should be of size $[N,1]$ and not $[N]$; in that case, \mintinline{python}{x.unsqueeze(-1)} can come in handy.

\subsubsection{Cross Entropy}
\begin{definition}[Cross Entropy]
    The \emph{cross entropy} is the loss function defined by:
    \begin{equation*}
        \begin{aligned}
            \ell : \R^C\times\iset{1}{C}&\longrightarrow\R\\
            x, y&\longmapsto -\log\left(\frac{\exp(x_y)}{\sum_i\exp(x_i)}\right)
        \end{aligned}
    \end{equation*}
\end{definition}

Similarly to MSE, minimizing the cross entropy corresponds to maximizing the $\log$-likelihood for a certain probabilistic model. Assume that there is a parameter $\theta\in\R^p$ such that, for all classes $k\in\iset{1}{C}$,
\begin{equation*}
    \log\P(Y_i=k|X_i) \propto g_\theta(X_i)_k
\end{equation*}
where $X_i$ are i.i.d.~and independent of $\theta$. Then, the likelihood for the data to be drawn for a given $\theta$ is:
\begin{equation*}
    \begin{aligned}
        \P_\theta((X_i,Y_i)_i) &= \prod_i\P(X_i)\cdot\P_\theta(Y_i|X_i)\\
        &\propto\prod_i\frac{\exp(g_\theta(X_i)_{Y_i})}{\sum_k\exp(g_\theta(X_i)_k)}        
    \end{aligned}
\end{equation*}

In \texttt{PyTorch}, cross entropy can be used with the line \mintinline{python}{criterion = nn.CrossEntropyLoss()}. It supports several parameters, such as \mintinline{python}{reduction='sum'}, \mintinline{python}{reduction='mean'}, which are described in \href{https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html}{the documentation}. Once set, \mintinline{python}{criterion} takes as input two tensors: the \emph{scores} (a tensor of shape $[N,C]$), and either a class index per sample, or class probabilities for each sample. Note that \mintinline{python}{nn.CrossEntropyLoss} is the composition of \mintinline{python}{nn.LogSoftmax} and \mintinline{python}{nn.NLLLoss}.

Finally, be aware that gradient can explode when going through a softmax, due to numerical errors. This is however taken care of by the \texttt{PyTorch} implementation of \mintinline{python}{nn.CrossEntropyLoss} and \mintinline{python}{nn.LogSoftmax}.

\subsection{First-order optimization}

\subsection{Convergence analysis}

\subsection{Gradient descent variants}