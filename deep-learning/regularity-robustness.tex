\section{Regularity and Robustness}
\subsection{Stability during training}
During the training of neural networks, the gradients can sometimes become very large, leading to a strong drop in accuracy. The final results can be quite random, and the final performance of the network depends on initialization.

\subsubsection{Gradient vanishing and gradient explosion}
This is mainly caused by two problems: gradient vanishing and gradient explosion, that we previously adressed in the specific case of RNNs and LSTM. Formaly, if $\theta_t$ are the iterates of the parameters learned using stochastic gradient descent on minibatches $(x_{t,i}, y_{t,i})_{i\in\iset{1}{K}}$ at time $t$, the parameter updates is:
\begin{equation*}
    \theta_{t+1}=\theta_t-\frac{\eta}{K}\sum_{i=1}^K \nabla\L_{x_{t,i}, y_{t,i}}(\theta)
\end{equation*}
where $\L_{x,y}(\theta)=\ell(g_\theta(x),y)$. When the gradients $\nabla\L_{x_{t,i}, y_{t,i}}(\theta)$ are very small compared to $\theta_t$, the iteration does not modify the parameters, and the optimization converges towards a non-minimal value. Alternatively, when the gradients are very large compared to $\theta_t$, the iteration will push the parameters to extreme values, and the optimization will not converge.

These two issues happen frequently in deep learning; backpropagation uses extensively the chain rule, causing the gradient to multiply along the layers. If we consider an estimator $g^{(L)}$ of the form:
\begin{equation*}
    g^{(L)}(x) = f^{(L)}\circ f^{(L-1)} \circ \dots\circ f^{(1)}(x)
\end{equation*}
where $f^{(l)}:\R\longrightarrow\R$, then:
\begin{equation*}
    g^{(L)'}(x) = \prod_{l=1}^L f^{(l)'} \circ g^{(l-1)}(x)
\end{equation*}
In this setup, if $f^{(l)'} \circ g^{(l-1)}(x) \simeq c$, then $g^{(L)'}(x) \simeq c^L$. Depending on the value of $c$, the gradient $g^{(L)'}(x)$ can become exponentially small w.r.t.~L (when $c<1$) or exponentially large (when $c>1$).

\subsubsection{Mitigation techniques}
We already explored diverse mitigation techniques to avoid these gradient problems. 

\begin{description}
    \item[Gradient clipping] The easiest method consists in limiting the gradient norm to a fixed value. It can be simply used in \texttt{PyTorch} with the following line:
    \begin{center}
        \mintinline{python}{torch.nn.utils.clip_grad_norm_(model.parameters(), threshold)}
    \end{center}
    Nevertheless, this is only for gradient explosion, and adds an extra hyperparameter.
    \item[Architecture changes] Another common approach to limit the gradient norm is to add changes to the network architecture. This is the use of gates in RNNs, residual connections in CNNs, and specialized layers such as dropout or batch normalization layers. This method is more principled and usually leads to better performance, but it does require to change the network architecture and is application dependent.
    \item[Weight initialization] We will analyze a complementary approach, \emph{weight initialization}: choosing \say{good} initial values for the parameters can have a large impact on performance, and can be automatically implemented.
\end{description}

\subsubsection{Weights initialization}
The intuitive idea is that the better the model is at initialization, the more changes we have to find good weights. We are not looking for the exact values of the parameters (this will be done automatically by the iterative gradient descent), but instead select an initial point that will lead to reasonable values of the weights.

In the following, we consider a fully-connected neural network $g_\theta$ with $\ReLU$ as activation function. 

What is the ideal initialization scheme? We would like to have values that are reasonable, that is:
\begin{equation*}
    \forall i\in\iset{i}{d^{(L)}}, \quad |g_\theta(x)_i| \simeq 1
\end{equation*}
We would also like to have gradients that are neither too large nor too small:
\begin{equation*}
    \forall i\in\iset{i}{p}, \quad \norm{\nabla\L_{x,y}(\theta)_i}\simeq 1
\end{equation*}

A simple solution is to set the biases $b^{(l)}=0$ and to sample the weights $W_{i,j}^{(l)}\sim\P$ i.i.d.~with expectation $0$ and variance $V^{(l)}$. We also choose $V^{(l)}$ so that the variance is constant across layers. Furthermore, we often choose a symmetric probability distribution with respect to 0, and such that the probability of 0 is null, to avoid ending up in a point where the gradient is null.

\begin{lemma}[Intermediate results are symmetric w.r.t.~0]
    Let $x\in\R^{d^(0)}$ be a fixed input and:
    \begin{equation}
        \forall l\in\iset{1}{L}, \quad X^{(l)}=g_\theta^{(2l-1)}(x)
    \end{equation}
    Then, for any $l\in\iset{1}{L}$, the variables $\left(X_i^{(l)}\right)_{i\in\iset{1}{d^{(2l-1)}}}$ are identically distributed. Furthermore, the distribution of $X_i^{(l)}$ is symmetric with respect to 0, and thus $\E\left[X_j^{(l)}\right]=0$.
\end{lemma}
\begin{proof}
    The proof follows a simple recurrence: $X_i^{(1)} = \sum_jW_{ij}^{(1)}x_j$ is identically distributed and symmetric; if the properties are verifies for $l$, then $X_i^{(l)} = \sum_jW_{ij}^{(l)}\ReLU(X_j^{(l-1)})$ which is identically distributed and symmetric.
\end{proof}

\begin{property}[Variance of the intermediate outputs]
    If $V^{(l)}=2/d^{(l-1)}$, the variance is constant across layers and:
    \begin{equation*}
        \V(g_\theta(x)_i) = \frac{2\norm{x}_2^2}{d^{(0)}}
    \end{equation*}
\end{property}
\begin{proof}
    For any $l\in\iset{2}{L}$ and $i\in\iset{1}{d^{(2l-1)}}$, we have:
    \begin{equation*}
        \begin{aligned}
            \V(X_i ^{(l)}) &= \V\left(\sum_jW_{ij}^{(l)}\ReLU\left(X_j^{(l-1)}\right)\right)\\
            &= \sum_j\V\left( W_{ij}^{(l)} \ReLU\left(X_j^{(l-1)}\right) \right)\\
            &= \V\left(W_{ij}^{(l)}\right) \cdot \E\left[\ReLU\left(X_j^{(l-1)}\right)^2\right] \cdot d^{(l-1)}\\
            &= V^{(l)} \cdot \E\left[X_j^{(l-1)^2} \cdot \ind_{X_j^{(l-1)}>0}\right] \cdot d^{(l-1)}\\
            &= V^{(l)} \cdot \frac{\V(X_j^{(l-1)})}{2} \cdot d^{(l-1)}
        \end{aligned}
    \end{equation*}
\end{proof}

\begin{property}[Variance of the intermediate gradients]
    Let $x\in\R^{d^{(0)}}$ be a fixed input and:
    \begin{equation}
        \forall l\in\iset{0}{2L-1}, \quad Y^{(l)}=J_{g_\theta^{(2l-1)}}(x)
    \end{equation}
    Then for any $l\in\iset{2}{L}$ and $i\in\iset{1}{d^{(2l-1)}}$, we have $Y^{(l)} = W^{(l)}D^{(l-1)}Y^{(l-1)}$. Furthermore, if $V^{l}=2/d^{(l-1)}$, the variance is constant across layers and:
    \begin{equation*}
        \V(J_{g_\theta}(x)_{ij}) = \frac{2}{d^{(0)}}
    \end{equation*}
\end{property}
\begin{proof}
    \begin{equation*}
        \begin{aligned}
            \V\left(Y_{ij}^{(l)}\right) &= \V\left(\sum_kW_{ik}^{(l)}\ReLU'\left(X_k^{(l-1)}\right) Y_{kj}^{(l-1)}\right)\\
            &= \sum_k \V\left(W_{ik}^{(l)} \ReLU'\left(X_k^{(l-1)}\right) Y_{kj}^{(l-1)}\right)\\
            &= d^{(l-1)} \cdot V^{(l)}\cdot \E\left[\ind_{X_k^{(l-1)}>0}\cdot Y_{kj}^{(l-1)^2}\right]\\
            &= d^{(l-1)} \cdot V^{(l)}\cdot \frac{\V\left(Y_{kj}^{(l-1)}\right)}{2}\\
        \end{aligned}
    \end{equation*}
\end{proof}

\subsubsection{Different initializations}
\paragraph*{Gaussian initialization} The previous assumptions are satisfied if we use Gaussian weights:
\begin{equation*}
    W_{ij}^{(l)} \sim \Nc\left(0, \frac{2}{d^{(l-1)}}\right)
\end{equation*}

\paragraph*{Uniform initialization}
If we take uniform weights:
\begin{equation*}
    W_{ij} \sim \mathcal{U}([-r^{(l)}, r^{(l)}])
\end{equation*}
then $V^{(l)}=r^2/3$ and $r^{(l)}=\sqrt{6/d^{(l-1)}}$.

 However, the gradient $\nabla\L_{x,y}(\theta)_i = \sum_iJ_{g_\theta}(x)_{ij}\nabla_x\ell(g_\theta(x),y)_j$ is a sum over $j\in\iset{1}{d^{(L)}}$. We thus prefere to set $V^{(l)}=2/d^{(l)}$ and have $\V(J_{g_\theta}(x)_{ij})=2/d^{(L)}$. In order to have both the variance of gradients and of values constant, we thus need $V^{(l)}=2/d^{(l)}=2/d^{(l-1)}$. A reasonable heuristic consists in taking the average:
\begin{equation*}
    V^{(l)} = \frac{4}{d^{(l)}+d^{(l-1)}}
\end{equation*}

\paragraph*{Xaxier initialization} Let $c>0$ be a hyperparameter. The weights are initialized using the heuristic:
\begin{equation*}
    W_{ij} \sim \mathcal{U}([-r^{(l)}, r^{(l)}]) \quad \textnormal{and} \quad r^{(l)}=\sqrt{\frac{6c^2}{d^{(l)}+d^{(l-1)}}}
\end{equation*}

\subsubsection{Batch normalization}
Another approach frequently used to increase stability during training is \emph{batch normalization}, previously discussed in the chapter about convolutional neural networks. Please refer to \autoref{subseq:normalization} for more information.

\subsection{Generalization beyond the training samples}

\subsection{Robustness and adversarial attacks}

\newpage